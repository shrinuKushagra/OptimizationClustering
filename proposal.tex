\documentclass{article}
\usepackage[nonatbib]{nips_2016}

\usepackage[breaklinks=true,letterpaper=true,colorlinks,citecolor=black,bookmarks=false]{hyperref}

%\usepackage{amsthm}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}

\usepackage[sort&compress,numbers]{natbib}
\usepackage[normalem]{ulem}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig} 

\graphicspath{{../fig/}}

\usepackage{tikz}
\usepackage{tkz-tab}
\usepackage{caption} 
\usepackage{subcaption} 
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{arrow} = [very thick,->,>=stealth]

\usepackage{cleveref}
\usepackage{setspace}
\usepackage{wrapfig}
%\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
%\usepackage[noend,linesnumbered]{algorithm2e}

\usepackage[disable]{todonotes}

\newcommand{\mc}{\mathcal}

\title{Proposal for CS798, Fall 2016\\ \large Optimization for Machine Learning}

\author{
	Nicole McNabb \\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{nmcnabb@uwaterloo.ca} \\
	\And
	Shrinu Kushagra\\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{skushagr@uwaterloo.ca} \\	
}

\begin{document}
\maketitle

\begin{abstract} 
Put here a brief summary of the project: what is it about? what is the main goal? how are you going to evaluate it? The proposal is expected to be 1-2 pages, so be concise and to the point.

In this project, we aim to formulate the $k$-median clustering objective as a linear program. We will then define a notion of "niceness" of data such that we can invent a polytime algorithm that returns an optimal, or close to optimal, clustering with high probability for "nice" datasets. 

We plan to evaluate our algorithm using worst-case analysis over the set of possible inputs that have the designated niceness property, and also by implementing the algorithm to show that it runs efficiently in practice over datasets with this niceness property.
\end{abstract} 

\section{Introduction}
In this section you are going to present a brief background and motivation about your project. Why is it interesting/significant? How does it relate to the course?

Clustering is a technique of unsupervised learning that attempts to group data points that are similar in features into "clusters". It can be used both as a tool to understand the underlying distibution of data or as a preprocessing step to alleviate the hardness of classification. While clustering methods such as $k$-means and $k$-median are efficient and widely used in practice, many clustering objectives are NP-hard to optimize over all data sets. Because of this, a primary aim of recent clustering research is to show instead that there exist clustering algorithms that perform efficiently over the set of all data sets that are likely to be seen in practice. Many of these data sets have some "nice" structure that would be useful to exploit. Ideally, we want a polytime algorithm that finds an optimal clustering for each dataset that possesses this "nice" structure.

In this project, we will specifically examine the $k$-median clustering problem. $K$-median can be formulated as a convex optimization problem, where the objective is to find $k$ cluster centers such that the total sum of the distance from each data point to its cluster center is minimized. We will attempt to formulate the $k$-median problem as a linear program with the Manhattan distance metric, then use this formulation to invent and implement an algorithm that finds an optimal or near-optimal clustering for the set of datasets that have some "nice" structure that will be later defined.

\section{Related Works}
Perform an initial review of relevant literature. Has your problem, or one of similar nature, been considered before? By whom? What are the differences or limitations (if any)? 

The $k$-means and $k$-median are by far the two most popular clustering algorithms. $K$-means tries to find a clustering which minimizes the following objective function
\begin{align*}
	\min_{c_1, \ldots, c_k} \sum_{i=1}^k \sum_{x \in C_i} \|x-c_i\|_2^2 
\end{align*}
The $k$-median objective is similar to the $k$-means objective. An important distinction is that the centers $c_1, \ldots, c_k$ are restricted to come from the input data $\mc X$. The optimization problem is
\begin{align*}
	\min_{c_1, \ldots, c_k \in \mc X} \sum_{i=1}^k \sum_{x \in C_i} \|x-c_i\|_2 
\end{align*}
Instead of the two-norm, other distance functions like $1$-norm etc. are also possible. Both the $k$-means and the $k$-median problem have been shown to be NP-hard. However, they have been successfully employed in practice on a lot of applications. Hence, recent research aims to bridge this gap by arguing that clustering is easy if the data has some nice properties. One of the such properties is $\alpha$-center proximity. 

A clustering $\mc C = \{C_1, \ldots, C_k\}$ satisfies $\alpha$-center proximity w.r.t $\mc X$ and $k$ if there exist centers $c_1, \ldots, c_k$  such that for all $x \in C_i$ and $i\neq j$, $$\alpha d(x, c_i) < d(x, c_j)$$

Researchers have shown that if the optimal clustering satisfies $\alpha$-center proximity then it is possible to find the optimal solution in polynomial time. 
\section{Proposed Work}
All the clustering algorithms discussed above are agnostic to the structure of the data. We want to pose an optimization problem which takes that into account. Clustering algorithm is essentially about choosing centers $c_1, \ldots, c_k$. What makes a "good" center? Intuitively a good center should have a lot of points close-by. Also, any two centers should be far apart from each other. We formulate the following optimization problem
\begin{align*}
	&\min_{c_1, \ldots, c_k \in \mc X} \sum_{i=1}^k \sum_{x \in N_t(c_i)} \|x-c_i\|_1 \\
	& \text{such that } \|c_i - c_j\|_1 \ge \alpha  \sum_{x \in N_t(c_i)} \|x-c_i\|_1 \text{ for all }i, j
\end{align*}

Here, parameters $\alpha < 1$ and $t > 1$ are known to the optimization algorithm. $\alpha$ represents the separation between the two clusters. The higher the $\alpha$, the larger is the separation. $t$ represents how dense the neighbourhood around a center $c_i$ is. 

For this project, we want to achieve the following:
\begin{itemize}
\item An algorithm to solve the above optimization problem OR prove that the optimization is NP-hard. 
\item If the above is (or "seems") NP-hard then propose a relaxation.
\item Show that if the data has some "nice" (suitable defined) structure, then our algorithm finds that structure.
\item Implement the algorithm on a dataset (such as MNIST or BagOfWords) and compare against other clustering algorithms.
\end{itemize}

%For your own sake, you might want to lay out a time line, so that you can keep a good track of your project.

\section{Team}
This project will be done by two people. Both the teammates will be equal contributors on the four directions mentioned above.

\bibliographystyle{unsrtnat}
\bibliography{proposal}

\end{document}