\documentclass{article}
\usepackage[nonatbib]{nips_2016}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage[breaklinks=true,letterpaper=true,colorlinks,citecolor=black,bookmarks=false]{hyperref}

\usepackage{amsmath,amssymb}
\usepackage{enumitem}

\usepackage[sort&compress,numbers]{natbib}
\usepackage[normalem]{ulem}

% use Times
\usepackage{times}

\usepackage{tikz}
\usepackage{tkz-tab}
\usepackage{caption} 
\usepackage{subcaption} 
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{arrow} = [very thick,->,>=stealth]

\usepackage{cleveref}
\usepackage{setspace}
\usepackage{wrapfig}
%\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
%\usepackage[noend,linesnumbered]{algorithm2e}

\usepackage[disable]{todonotes}
\newcommand{\mc}{\mathcal}

\title{Proposal for CS798, Fall 2016\\ \large Optimization for Machine Learning}

\author{
	Nicole McNabb \\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{nmcnabb@uwaterloo.ca} \\
	\And
	Shrinu Kushagra\\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{skushagr@uwaterloo.ca} \\	
}

\begin{document}
\maketitle

\begin{abstract} 
In this project, we will formulate the $k$-median clustering objective as a linear program. We will then define a notion of "niceness" of data that we hope will allow us to create a polytime algorithm that with high probability, returns an optimal or near-optimal clustering for datasets that satisfy the niceness notion.

We plan to evaluate this algorithm using worst-case analysis over the set of possible inputs with the niceness property, and also by implementing the algorithm to show that it runs efficiently in practice over datasets with this niceness property.
\end{abstract} 

\section{Introduction}
Clustering is an unsupervised learning technique that groups data points that have similar features into "clusters". It can be used both as a tool to understand the underlying distibution of a dataset or as a preprocessing step to alleviate the hardness of classification. While it is known that many clustering objectives are NP-hard to optimize over all data sets, in practice, clustering methods such as $k$-means and $k$-median are very efficient. This means that datasets clustered in practice likely have some structure that allows efficient clustering. This idea has led to an area of recent clustering research that aims to show there exist clustering algorithms that perform efficiently over the set of all data sets that are likely to be seen in practice. This means that for each dataset that has some predefined structure, researchers believe there exists a polytime algorithm that finds an optimal clustering. We aim to gain some insight as to how such an algorithm might look, if one does exist.

In this project, we examine the $k$-median clustering problem. We will show it can be formulated as a convex optimization problem, and formulate it as a linear program (with a linear objective function and linear constraints). We hope to use this formulation to invent and implement an algorithm that finds an optimal or near-optimal clustering for datasets that have some "nice" structure we later define.

\section{Related Works}
$K$-means and $k$-median are by far the two most widely used clustering algorithms in practice. $K$-means attempts to find a clustering that minimizes the objective function
\begin{align*}
	\min_{c_1, \ldots, c_k} \sum_{i=1}^k \sum_{x \in C_i} \|x-c_i\|_2^2 .
\end{align*}

The $k$-median objective is similar to the $k$-means objective. However, one important distinction is that in $k$-median, the centers $c_1, \ldots, c_k$ must come from the input data $\mc X$. The objective function for $k$-median is
\begin{align*}
	\min_{c_1, \ldots, c_k \in \mc X} \sum_{i=1}^k \sum_{x \in C_i} \|x-c_i\|_1 ,
\end{align*}
because the $1$-norm Manhattan distance metric is most commonly used. However, the $2$-norm Euclidean distance metric can also be used. 

Both the $k$-means and the $k$-median problem have been shown to be NP-hard \cite{kmeans}\cite{kmeans2}\cite{kmed}. However, they have been successfully employed in practice on a lot of applications. Hence, recent research aims to bridge this gap by arguing that clustering is easy if the data has some nice properties. One of these properties is $\alpha$-center proximity \cite{alphacenter}. 

A clustering $\mc C = \{C_1, \ldots, C_k\}$ satisfies $\alpha$-center proximity with respect to $\mc X$ and $k$ if there exist centers $c_1, \ldots, c_k$  such that for all $x \in C_i$ and $i\neq j$, $$\alpha d(x, c_i) < d(x, c_j).$$

It has been shown that if the optimal clustering satisfies $\alpha$-center proximity then it is possible to find the optimal solution in polynomial time \cite{Balcan}. The polytime algorithm works by building a hierarchical clustering tree on the input and then finding a pruning of minimum cost. However, the notion of $\alpha$-center proximity is too restrictive. Recent attempts have been made to relax it by adding noise to the data \cite{noise1}\cite{noise2}. In this work, we wish to study clustering as an optimization problem in the presence of such noisy points.  

\section{Proposed Work}
All the clustering algorithms discussed above are agnostic to the structure of the data. We want to pose an optimization problem which takes that into account. A $k$-median algorithm aims to choose "good" centers $c_1, \ldots, c_k$. What makes a "good" center? Intuitively, a good center should have a lot of points closeby and should have all other centers be far away. We formulate the following optimization problem
\begin{align*}
	&\min_{c_1, \ldots, c_k \in \mc X} \sum_{i=1}^k \sum_{x \in N_t(c_i)} \|x-c_i\|_1 \\
	& \text{such that } \|c_i - c_j\|_1 \ge \alpha\cdot\sum_{x \in N_t(c_i)} \|x-c_i\|_1 \text{ for all }i, j,
\end{align*}

where $N_t(c_i)$ is the set of the $t$ nearest neighbours of center $c_i$. $\alpha$ represents the degree of separation between two clusters, where a higher value of $\alpha$ means a larger separation. Parameters $\alpha < 1$ and $t \geq 1$ will be known to the algorithm.

In this project, we want to achieve the following:
\begin{itemize}
\item An algorithm to solve the above optimization problem OR prove that the optimization is NP-hard. 
\item If the above is (or "seems") NP-hard then propose a relaxation.
\item Show that if the data has some "nice" (suitably defined) structure, then our algorithm finds that structure.
\item Implement the algorithm on a dataset (such as MNIST or BagOfWords) and compare against other clustering algorithms.
\end{itemize}

\section{Team}
This project will be done by two people. Both the teammates will be equal contributors on the four directions mentioned above.

\bibliographystyle{unsrtnat}
\bibliography{proposal}
\end{document}